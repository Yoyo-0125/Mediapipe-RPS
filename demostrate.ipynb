{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9f0695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75214fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_transform(keypoints):\n",
    "    # 1. 将一维数组 (63,) 重塑为 (21, 3) 的矩阵\n",
    "    kp_matrix = torch.tensor(keypoints, dtype=torch.float32).reshape(21, 3)\n",
    "    # 2. 获取手腕坐标 (第0行)\n",
    "    wrist = kp_matrix[0].clone()  # Shape: (3,)\n",
    "    # 3. 所有点减去手腕坐标 (广播机制，自动应用到每一行)\n",
    "    kp_matrix -= wrist\n",
    "    # 4. 归一化 (保持原有逻辑，缩放到 -1 到 1)\n",
    "    # 注意：这里建议用所有相对坐标的最大值来缩放，保持长宽比\n",
    "    max_val = torch.max(torch.abs(kp_matrix))\n",
    "    if max_val > 0:\n",
    "        kp_matrix /= max_val\n",
    "    # 5. 展平回 (63,)\n",
    "    return kp_matrix.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43fc77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(63, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16d01fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=63, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), \"mlp_model_weights.pth\")\n",
    "\n",
    "model = MLP().to(device)\n",
    "model.load_state_dict(torch.load(\"mlp_model_weights.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e567fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# ---------- 加载 PNG ----------\n",
    "imgs = {\n",
    "    0: cv2.imread(\"images\\\\rock.png\", cv2.IMREAD_UNCHANGED),\n",
    "    1: cv2.imread(\"images\\\\paper.png\", cv2.IMREAD_UNCHANGED), \n",
    "    2: cv2.imread(\"images\\\\scissor.png\", cv2.IMREAD_UNCHANGED)\n",
    "}\n",
    "\n",
    "# ---------- 反手势规则 ----------\n",
    "opposite = {0: 1, 1: 2, 2: 0}\n",
    "\n",
    "# ---------- Mediapipe ----------\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# ---------- PNG 叠加函数 ----------\n",
    "def overlay_png(background, png, x, y):\n",
    "    h, w = png.shape[:2]\n",
    "    bg_h, bg_w = background.shape[:2]\n",
    "    if x + w > bg_w or y + h > bg_h:\n",
    "        return background\n",
    "    bgr = png[:, :, :3]\n",
    "    alpha = png[:, :, 3] / 255.0\n",
    "    for c in range(3):\n",
    "        background[y:y+h, x:x+w, c] = (\n",
    "            bgr[:, :, c] * alpha\n",
    "            + background[y:y+h, x:x+w, c] * (1 - alpha)\n",
    "        )\n",
    "    return background\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # frame = cv2.flip(frame, 1)\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        keypoints = []\n",
    "        for lm in hand_landmarks.landmark:\n",
    "            keypoints.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "        keypoints = keypoints_transform(keypoints).unsqueeze(0).to(device)\n",
    "        res = model(keypoints)\n",
    "        predicted_class = torch.argmax(res, dim=1).item()\n",
    "        class_names = [\"Rock\", \"Paper\", \"Scissors\"]\n",
    "        cv2.putText(frame, f'Prediction: {class_names[predicted_class]}', (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        frame = overlay_png(frame, imgs[opposite[predicted_class]], 10, 50)\n",
    "        \n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            h, w, c = frame.shape\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            x_list = []\n",
    "            y_list = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                x_list.append(cx)\n",
    "                y_list.append(cy)\n",
    "\n",
    "            x_min, x_max = min(x_list), max(x_list)\n",
    "            y_min, y_max = min(y_list), max(y_list)\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0,255,0), 2)\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25273768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
